# llama.cpp åç«¯å‡çº§åˆ†ææŠ¥å‘Š
## ç‰ˆæœ¬è¿ç§»ï¼šb5421 â†’ b7785

**ç”Ÿæˆæ—¥æœŸ**: 2026-01-20
**åˆ†æå¯¹è±¡**: localLLM R Package
**å½“å‰ç‰ˆæœ¬**: b5421 (d30cb5a7f, 2025-05-19)
**ç›®æ ‡ç‰ˆæœ¬**: b7785 (1c7cf94b2, 2026-01-20)
**æäº¤è·¨åº¦**: 2,364 commits (8ä¸ªæœˆ)

---

## æ‰§è¡Œæ‘˜è¦

### âœ… **å¥½æ¶ˆæ¯ï¼šR å±‚ä»£ç æ— éœ€ä¿®æ”¹**

ç»è¿‡è¯¦ç»†åˆ†æï¼Œä½ çš„ R package æ¶æ„è®¾è®¡éå¸¸ä¼˜ç§€ï¼Œé‡‡ç”¨äº†**æŠ½è±¡çš„ C API å±‚** (`localllm_capi.h`)ï¼Œè¿™ä½¿å¾—å‡çº§è¿‡ç¨‹ç›¸å¯¹ç®€å•ï¼š

- âœ… **R å±‚ä»£ç  (R/*.R)**: **å®Œå…¨ä¸éœ€è¦ä¿®æ”¹**
- âœ… **C API å®šä¹‰ (localllm_capi.h)**: **ä¸éœ€è¦ä¿®æ”¹**
- âš ï¸ **åç«¯å®ç°åº“**: **éœ€è¦é‡æ–°ç¼–è¯‘**ï¼ˆé’ˆå¯¹ b7785ï¼‰
- ğŸ” **å¯é€‰æ”¹è¿›**: å¯ä»¥åˆ©ç”¨ b7785 çš„æ–°åŠŸèƒ½ä¼˜åŒ–æ€§èƒ½

### ğŸ¯ **æ ¸å¿ƒå‘ç°**

ä½ çš„ R package å½“å‰**æ²¡æœ‰ä½¿ç”¨ä»»ä½• KV cache API**ï¼Œè¿™æ˜¯æœ€å¤§çš„å¥½æ¶ˆæ¯ï¼æ‰€æœ‰æš´éœ²çš„å‡½æ•°éƒ½æ˜¯é«˜å±‚å°è£…ï¼š

- âœ… æ²¡æœ‰ `llama_kv_self_*` è°ƒç”¨
- âœ… æ²¡æœ‰ç›´æ¥æ“ä½œ `llama_context_params` æˆ– `llama_model_params` ç»“æ„ä½“
- âœ… æ‰€æœ‰å‚æ•°éƒ½é€šè¿‡å‡½æ•°å‚æ•°ä¼ é€’ï¼Œè€Œéç»“æ„ä½“
- âœ… ä½¿ç”¨äº† opaque æŒ‡é’ˆ (`localllm_model_handle`, `localllm_context_handle`)

---

## å½“å‰ API ä½¿ç”¨æƒ…å†µåˆ†æ

### 1. æš´éœ²çš„ C API å‡½æ•°ï¼ˆå…± 34 ä¸ªï¼‰

æ ¹æ® `localllm/src/localllm_capi.h` å’Œ `proxy.h`ï¼Œå½“å‰æš´éœ²çš„å‡½æ•°ä¸ºï¼š

| ç±»åˆ« | å‡½æ•°å | b7785 å…¼å®¹æ€§ |
|------|--------|-------------|
| **åç«¯ç®¡ç†** (2) | | |
| | `localllm_backend_init()` | âœ… å…¼å®¹ |
| | `localllm_backend_free()` | âœ… å…¼å®¹ |
| **æ¨¡å‹ç®¡ç†** (3) | | |
| | `localllm_model_load()` | âœ… å…¼å®¹ |
| | `localllm_model_load_safe()` | âœ… å…¼å®¹ |
| | `localllm_model_free()` | âœ… å…¼å®¹ |
| **ä¸Šä¸‹æ–‡ç®¡ç†** (2) | | |
| | `localllm_context_create()` | âœ… å…¼å®¹ |
| | `localllm_context_free()` | âœ… å…¼å®¹ |
| **æ–‡æœ¬å¤„ç†** (5) | | |
| | `localllm_tokenize()` | âœ… å…¼å®¹ |
| | `localllm_detokenize()` | âœ… å…¼å®¹ |
| | `localllm_apply_chat_template()` | âœ… å…¼å®¹ |
| | `localllm_generate()` | âœ… å…¼å®¹ |
| | `localllm_generate_parallel()` | âœ… å…¼å®¹ |
| **å†…å­˜ç®¡ç†** (3) | | |
| | `localllm_free_tokens()` | âœ… å…¼å®¹ |
| | `localllm_free_string()` | âœ… å…¼å®¹ |
| | `localllm_free_string_array()` | âœ… å…¼å®¹ |
| **Token æŸ¥è¯¢** (16) | | |
| | `localllm_token_get_text()` | âœ… å…¼å®¹ |
| | `localllm_token_bos()` | âœ… å…¼å®¹ |
| | `localllm_token_eos()` | âœ… å…¼å®¹ |
| | `localllm_token_sep()` | âœ… å…¼å®¹ |
| | `localllm_token_nl()` | âœ… å…¼å®¹ |
| | `localllm_token_pad()` | âœ… å…¼å®¹ |
| | `localllm_token_eot()` | âœ… å…¼å®¹ |
| | `localllm_add_bos_token()` | âœ… å…¼å®¹ |
| | `localllm_add_eos_token()` | âœ… å…¼å®¹ |
| | `localllm_token_fim_pre()` | âœ… å…¼å®¹ |
| | `localllm_token_fim_mid()` | âœ… å…¼å®¹ |
| | `localllm_token_fim_suf()` | âœ… å…¼å®¹ |
| | `localllm_token_get_attr()` | âœ… å…¼å®¹ |
| | `localllm_token_get_score()` | âœ… å…¼å®¹ |
| | `localllm_token_is_eog()` | âœ… å…¼å®¹ |
| | `localllm_token_is_control()` | âœ… å…¼å®¹ |
| **ä¸‹è½½/è§£æ** (2) | | |
| | `localllm_download_model()` | âœ… å…¼å®¹ |
| | `localllm_resolve_model()` | âœ… å…¼å®¹ |
| **å†…å­˜æ£€æŸ¥** (2) | | |
| | `localllm_estimate_model_memory()` | âœ… å…¼å®¹ |
| | `localllm_check_memory_available()` | âœ… å…¼å®¹ |

**ç»“è®º**: æ‰€æœ‰ 34 ä¸ªå‡½æ•°åœ¨ b7785 ä¸­çš„åº•å±‚ llama.cpp API **å‡ä¿æŒå…¼å®¹**ã€‚

### 2. æœªä½¿ç”¨çš„ llama.cpp API

ä½ çš„ R package **å®Œå…¨æ²¡æœ‰ä½¿ç”¨**ä»¥ä¸‹è¢«é‡æ„çš„ APIï¼ˆè¿™æ˜¯å¥½æ¶ˆæ¯ï¼ï¼‰ï¼š

- âŒ `llama_kv_self_*` ç³»åˆ—å‡½æ•°ï¼ˆå·²æ”¹ä¸º `llama_memory_*`ï¼‰
- âŒ `llama_kv_cache_*` ç³»åˆ—å‡½æ•°ï¼ˆå·²å¼ƒç”¨ï¼‰
- âŒ ç›´æ¥æ“ä½œ `llama_context_params` æˆ– `llama_model_params` ç»“æ„ä½“
- âŒ `llama_kv_cache_view_*` å‡½æ•°ï¼ˆå·²ç§»é™¤ï¼‰

---

## åç«¯å®ç°åº“éœ€è¦çš„è°ƒæ•´

è™½ç„¶ C API æ¥å£å±‚ä¸éœ€è¦ä¿®æ”¹ï¼Œä½†åç«¯å®ç°åº“ï¼ˆä½ éœ€è¦é‡æ–°ç¼–è¯‘çš„éƒ¨åˆ†ï¼‰éœ€è¦é€‚é… b7785 çš„å˜åŒ–ã€‚

### å…³é”®è°ƒæ•´ç‚¹

#### 1. **å‚æ•°ç»“æ„ä½“åˆå§‹åŒ–**ï¼ˆå¿…é¡»ä¿®æ”¹ï¼‰

**å½“å‰å¯èƒ½çš„å®ç°æ–¹å¼ï¼ˆb5421ï¼‰ï¼š**
```cpp
// åç«¯åº“ä¸­çš„ localllm_model_load_safe å®ç°
llama_model_params params;
params.n_gpu_layers = n_gpu_layers;
params.use_mmap = use_mmap;
params.use_mlock = use_mlock;
// ...å…¶ä»–å­—æ®µå¯èƒ½æœªåˆå§‹åŒ–
```

**å¿…é¡»æ”¹ä¸ºï¼ˆb7785ï¼‰ï¼š**
```cpp
// ä½¿ç”¨é»˜è®¤å‚æ•°åˆå§‹åŒ–
llama_model_params params = llama_model_default_params();

// ç„¶åè¦†ç›–ç”¨æˆ·æä¾›çš„å€¼
params.n_gpu_layers = n_gpu_layers;
params.use_mmap = use_mmap;
params.use_mlock = use_mlock;
```

**åŸå› **: b7785 æ–°å¢äº†ä»¥ä¸‹å­—æ®µï¼Œæœªåˆå§‹åŒ–ä¼šå¯¼è‡´æœªå®šä¹‰è¡Œä¸ºï¼š
- `bool use_direct_io`
- `bool use_extra_bufts`
- `bool no_host`
- `bool no_alloc`

#### 2. **ä¸Šä¸‹æ–‡å‚æ•°ç»“æ„ä½“åˆå§‹åŒ–**ï¼ˆå¿…é¡»ä¿®æ”¹ï¼‰

**ç±»ä¼¼çš„ï¼Œ`llama_context_params` ä¹Ÿéœ€è¦é»˜è®¤åˆå§‹åŒ–ï¼š**

```cpp
// b7785 å¿…é¡»ä½¿ç”¨
llama_context_params cparams = llama_context_default_params();

// ç„¶åè®¾ç½®ç”¨æˆ·å‚æ•°
cparams.n_ctx = n_ctx;
cparams.n_threads = n_threads;
// ...
```

**åŸå› **: b7785 æ–°å¢å­—æ®µï¼š
- `enum llama_flash_attn_type flash_attn_type`ï¼ˆæ›¿ä»£æ—§çš„ `bool flash_attn`ï¼‰
- `bool swa_full`
- `bool kv_unified`
- `struct llama_sampler_seq_config * samplers`
- `size_t n_samplers`

#### 3. **Flash Attention é…ç½®**ï¼ˆå¯é€‰æ”¹è¿›ï¼‰

å¦‚æœåç«¯åº“ä¹‹å‰ä½¿ç”¨äº† `flash_attn` å­—æ®µï¼š

```cpp
// b5421 (æ—§)
cparams.flash_attn = true;  // bool ç±»å‹

// b7785 (æ–°)
cparams.flash_attn_type = LLAMA_FLASH_ATTN_TYPE_AUTO;  // enum ç±»å‹
```

**å»ºè®®**: ä½¿ç”¨ `AUTO` æ¨¡å¼è®© llama.cpp è‡ªåŠ¨å†³å®šæ˜¯å¦å¯ç”¨ã€‚

#### 4. **llama_decode() é”™è¯¯å¤„ç†**ï¼ˆå»ºè®®æ”¹è¿›ï¼‰

**å½“å‰å¯èƒ½çš„å®ç°ï¼š**
```cpp
int ret = llama_decode(ctx, batch);
if (ret != 0) {
    return LOCALLLM_ERROR;
}
```

**å»ºè®®æ”¹è¿›ï¼ˆb7785ï¼‰ï¼š**
```cpp
int ret = llama_decode(ctx, batch);
if (ret == 1) {
    *error_message = "Could not find KV slot - try reducing batch size or increasing n_ctx";
    return LOCALLLM_ERROR;
} else if (ret == 2) {
    // ä¸­æ­¢ï¼Œä½†éƒ¨åˆ†æ•°æ®å·²å¤„ç† - å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
    *error_message = "Decoding aborted - partial results available";
    return LOCALLLM_ERROR;
} else if (ret == -1) {
    *error_message = "Invalid input batch";
    return LOCALLLM_ERROR;
} else if (ret < -1) {
    *error_message = "Fatal error during decoding";
    return LOCALLLM_ERROR;
}
return LOCALLLM_SUCCESS;
```

---

## è¿ç§»æ­¥éª¤æ¸…å•

### é˜¶æ®µ 1ï¼šå‡†å¤‡å·¥ä½œ âœ…

- [x] å¤‡ä»½å½“å‰ä»£ç 
- [x] åˆ†æ API å…¼å®¹æ€§
- [x] ç¡®è®¤æ²¡æœ‰ä½¿ç”¨ KV cache API
- [x] ç¡®è®¤æ‰€æœ‰å‡½æ•°ç­¾åå…¼å®¹

### é˜¶æ®µ 2ï¼šåç«¯åº“æ›´æ–° ğŸ”§

#### æ­¥éª¤ 2.1ï¼šåˆ‡æ¢åˆ° b7785

```bash
cd backend/llama.cpp
git checkout b7785
```

#### æ­¥éª¤ 2.2ï¼šæ›´æ–°åç«¯å®ç°ä»£ç 

**æ–‡ä»¶ä½ç½®**: `backend/llama.cpp/examples/` æˆ–ä½ è‡ªå®šä¹‰çš„åç«¯å®ç°ç›®å½•

**å¿…é¡»ä¿®æ”¹çš„åœ°æ–¹ï¼š**

1. **æ¨¡å‹åŠ è½½å‡½æ•°** (`localllm_model_load` / `localllm_model_load_safe`)
   ```cpp
   // æ·»åŠ è¿™ä¸€è¡Œ
   llama_model_params params = llama_model_default_params();

   // ç„¶åè®¾ç½®ç”¨æˆ·å‚æ•°
   params.n_gpu_layers = n_gpu_layers;
   params.use_mmap = use_mmap;
   params.use_mlock = use_mlock;
   params.vocab_only = false;

   // è°ƒç”¨ llama.cpp API
   llama_model* model = llama_model_load_from_file(model_path, params);
   ```

2. **ä¸Šä¸‹æ–‡åˆ›å»ºå‡½æ•°** (`localllm_context_create`)
   ```cpp
   // æ·»åŠ è¿™ä¸€è¡Œ
   llama_context_params cparams = llama_context_default_params();

   // è®¾ç½®ç”¨æˆ·å‚æ•°
   cparams.n_ctx = n_ctx;
   cparams.n_threads = n_threads;
   cparams.n_seq_max = n_seq_max;

   // å¯é€‰ï¼šå¯ç”¨ Flash Attention
   cparams.flash_attn_type = LLAMA_FLASH_ATTN_TYPE_AUTO;

   // è°ƒç”¨ llama.cpp API
   llama_context* ctx = llama_context_new_from_model(model, cparams);
   ```

3. **é”™è¯¯å¤„ç†æ”¹è¿›** (åœ¨ `localllm_generate` å’Œ `localllm_generate_parallel` ä¸­)
   ```cpp
   int ret = llama_decode(ctx, batch);
   if (ret != 0) {
       // æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
       switch (ret) {
           case 1:
               *error_message = "No KV slot available";
               break;
           case 2:
               *error_message = "Decoding aborted";
               break;
           case -1:
               *error_message = "Invalid batch";
               break;
           default:
               *error_message = "Fatal error";
       }
       return LOCALLLM_ERROR;
   }
   ```

#### æ­¥éª¤ 2.3ï¼šé‡æ–°ç¼–è¯‘åç«¯åº“

```bash
cd backend/llama.cpp
mkdir -p build && cd build

# macOS ARM64 ç¤ºä¾‹
cmake .. \
  -DCMAKE_BUILD_TYPE=Release \
  -DGGML_METAL=ON \
  -DBUILD_SHARED_LIBS=ON

cmake --build . --config Release -j $(sysctl -n hw.ncpu)

# è¾“å‡ºåº”è¯¥æ˜¯ liblocalllm.dylib (macOS) æˆ– liblocalllm.so (Linux)
```

#### æ­¥éª¤ 2.4ï¼šæ‰“åŒ…æ–°çš„åç«¯åº“

å°†ç¼–è¯‘å¥½çš„åº“æ–‡ä»¶å¤åˆ¶åˆ°å‘å¸ƒä½ç½®ï¼š

```bash
# æ ¹æ®ä½ çš„å‘å¸ƒæµç¨‹ï¼Œå¯èƒ½éœ€è¦ï¼š
# 1. é‡å‘½åä¸º liblocalllm_macos_arm64.dylib
# 2. å‹ç¼©ä¸º .zip
# 3. ä¸Šä¼ åˆ° GitHub Releases
```

### é˜¶æ®µ 3ï¼šæµ‹è¯• ğŸ§ª

#### 3.1 å•å…ƒæµ‹è¯•

```r
# åœ¨ R ä¸­æµ‹è¯•
library(localLLM)

# å®‰è£…æ–°çš„åç«¯åº“
install_localLLM()

# æµ‹è¯•åŸºç¡€åŠŸèƒ½
model <- model_load("path/to/model.gguf")
ctx <- context_create(model, n_ctx = 2048)

# æµ‹è¯•ç”Ÿæˆ
result <- generate(ctx, "Hello", max_tokens = 10)
print(result)

# æµ‹è¯•å¹¶è¡Œç”Ÿæˆ
results <- generate_parallel(ctx, c("Hello", "Hi"), max_tokens = 10)
print(results)
```

#### 3.2 å›å½’æµ‹è¯•

```bash
# è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶
cd localLLM
R CMD check .
```

#### 3.3 æ€§èƒ½æµ‹è¯•

```r
# å¯¹æ¯” b5421 vs b7785 çš„æ€§èƒ½
library(microbenchmark)

microbenchmark(
  generate(ctx, "Test prompt", max_tokens = 100),
  times = 10
)
```

### é˜¶æ®µ 4ï¼šæ–‡æ¡£æ›´æ–° ğŸ“

- [ ] æ›´æ–° `README.md` ä¸­çš„ç‰ˆæœ¬ä¿¡æ¯
- [ ] æ›´æ–° `NEWS.md` / `CHANGELOG.md`
- [ ] æ›´æ–° `DESCRIPTION` æ–‡ä»¶çš„ç‰ˆæœ¬å·
- [ ] æ›´æ–°åç«¯åº“çš„ GitHub Release æ ‡ç­¾

---

## é£é™©è¯„ä¼°

| é£é™©é¡¹ | æ¦‚ç‡ | å½±å“ | ç¼“è§£æªæ–½ |
|-------|------|------|---------|
| **å‚æ•°ç»“æ„ä½“æœªåˆå§‹åŒ–** | ğŸŸ¡ ä¸­ | ğŸ”´ é«˜ | ä½¿ç”¨ `default_params()` å‡½æ•° |
| **ç¼–è¯‘é”™è¯¯** | ğŸŸ¢ ä½ | ğŸŸ¡ ä¸­ | æå‰åœ¨æœ¬åœ°æµ‹è¯•ç¼–è¯‘ |
| **è¿è¡Œæ—¶å´©æºƒ** | ğŸŸ¢ ä½ | ğŸ”´ é«˜ | å……åˆ†æµ‹è¯•æ‰€æœ‰å‡½æ•° |
| **æ€§èƒ½å›é€€** | ğŸŸ¢ ä½ | ğŸŸ¡ ä¸­ | b7785 æ•´ä½“æ€§èƒ½ä¼˜äº b5421 |
| **Token å‡½æ•°è¡Œä¸ºå˜åŒ–** | ğŸŸ¢ ä½ | ğŸŸ¢ ä½ | API ç­¾åæœªå˜ |
| **å†…å­˜æ³„æ¼** | ğŸŸ¢ ä½ | ğŸŸ¡ ä¸­ | ä½¿ç”¨ valgrind/ASAN æµ‹è¯• |

**æ€»ä½“é£é™©ç­‰çº§**: ğŸŸ¡ **ä¸­ä½é£é™©**

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

å‡çº§åˆ° b7785 åï¼Œå¯ä»¥åˆ©ç”¨ä»¥ä¸‹æ–°ç‰¹æ€§æå‡æ€§èƒ½ï¼š

### 1. **å¯ç”¨ Flash Attention**

```cpp
// åœ¨ localllm_context_create å®ç°ä¸­
cparams.flash_attn_type = LLAMA_FLASH_ATTN_TYPE_AUTO;
```

**é¢„æœŸæ”¶ç›Š**: é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹ **20-40% é€Ÿåº¦æå‡**

### 2. **ä½¿ç”¨å‚æ•°è‡ªé€‚åº”**ï¼ˆæœªæ¥åŠŸèƒ½ï¼‰

å¯ä»¥è€ƒè™‘åœ¨ R å±‚æš´éœ² `llama_params_fit()` å‡½æ•°ï¼š

```cpp
// æ–°çš„ C API å‡½æ•°
localllm_error_code localllm_params_fit(
    const char* model_path,
    int* n_gpu_layers_out,
    int* n_ctx_out,
    const char** error_message
);
```

**ç”¨é€”**: è‡ªåŠ¨è®¡ç®—è®¾å¤‡å¯ä»¥æ”¯æŒçš„æœ€å¤§ `n_ctx` å’Œ `n_gpu_layers`

### 3. **ç»Ÿä¸€ KV Buffer**ï¼ˆå¤šåºåˆ—åœºæ™¯ï¼‰

å¦‚æœç”¨æˆ·ä½¿ç”¨ `n_seq_max > 1` è¿›è¡Œæ‰¹é‡æ¨ç†ï¼š

```cpp
cparams.kv_unified = true;  // å…±äº«å‰ç¼€ç¼“å­˜
```

**é¢„æœŸæ”¶ç›Š**: å¤šåºåˆ—åœºæ™¯ä¸‹å†…å­˜ä½¿ç”¨å‡å°‘ **30-50%**

---

## é™„å½• Aï¼šåç«¯åº“æ–‡ä»¶æ£€æŸ¥æ¸…å•

è¯·ç¡®è®¤ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨å¹¶åŒ…å«æ­£ç¡®çš„å®ç°ï¼š

### æ–‡ä»¶ç»“æ„ï¼ˆæ¨æµ‹ï¼‰

```
backend/llama.cpp/
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ localllm/           # ä½ çš„åç«¯å®ç°ï¼ˆå¯èƒ½åœ¨è¿™é‡Œï¼‰
â”‚       â”œâ”€â”€ localllm.cpp    # å®ç°æ‰€æœ‰ localllm_* å‡½æ•°
â”‚       â”œâ”€â”€ CMakeLists.txt  # æ„å»ºé…ç½®
â”‚       â””â”€â”€ ...
â”œâ”€â”€ include/
â”‚   â””â”€â”€ llama.h             # b7785 ç‰ˆæœ¬
â””â”€â”€ ...
```

### éœ€è¦æ£€æŸ¥çš„å‡½æ•°å®ç°

åœ¨åç«¯åº“æºç ä¸­æœç´¢ä»¥ä¸‹å‡½æ•°ï¼Œç¡®è®¤å®ƒä»¬ï¼š

1. âœ… ä½¿ç”¨ `llama_model_default_params()`
2. âœ… ä½¿ç”¨ `llama_context_default_params()`
3. âœ… æ²¡æœ‰ä½¿ç”¨å·²ç§»é™¤çš„ `llama_kv_self_*` å‡½æ•°
4. âœ… æ­£ç¡®å¤„ç† `llama_decode()` è¿”å›å€¼

### æ£€æŸ¥å‘½ä»¤

```bash
cd backend/llama.cpp
grep -r "llama_model_params" examples/ custom/
grep -r "llama_context_params" examples/ custom/
grep -r "llama_kv_self" examples/ custom/  # åº”è¯¥æ²¡æœ‰ç»“æœ
```

---

## é™„å½• Bï¼šb7785 çš„é‡å¤§æ”¹è¿›

å‡çº§åä½ å°†è·å¾—çš„å¥½å¤„ï¼š

### 1. **æ€§èƒ½æ”¹è¿›**

- âœ… Flash Attention 2.0 ä¼˜åŒ–ï¼ˆæ›´å¿«çš„æ³¨æ„åŠ›æœºåˆ¶ï¼‰
- âœ… æ›´å¥½çš„å†…å­˜ç¢ç‰‡æ•´ç†ï¼ˆè‡ªåŠ¨åŒ–ï¼‰
- âœ… KV cache â†’ Memory æŠ½è±¡ï¼ˆæ”¯æŒ Hybrid cacheï¼‰
- âœ… æ”¹è¿›çš„æ‰¹å¤„ç†æ€§èƒ½

### 2. **æ–°æ¨¡å‹æ”¯æŒ**

- âœ… Llama 4 ç³»åˆ—æ¨¡å‹
- âœ… DeepSeek 3 LLM
- âœ… Pixtral å¤šæ¨¡æ€æ¨¡å‹
- âœ… Gemma, Qwen, æ›´å¤š MoE æ¨¡å‹

### 3. **é‡åŒ–æ”¯æŒ**

- âœ… æ–°å¢ MXFP4_MOE é‡åŒ–ï¼ˆMoE æ¨¡å‹ä¸“ç”¨ï¼‰
- âœ… æ”¹è¿›çš„ TQ1_0/TQ2_0 é‡åŒ–

### 4. **å¼€å‘è€…ä½“éªŒ**

- âœ… æ›´æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯
- âœ… æ›´ç»†ç²’åº¦çš„æ—¥å¿—æ§åˆ¶
- âœ… æ›´å¥½çš„å†…å­˜ä½¿ç”¨ç›‘æ§

---

## é™„å½• Cï¼šå¿«é€ŸéªŒè¯è„šæœ¬

### C++ åç«¯éªŒè¯

åˆ›å»º `test_backend.cpp`ï¼š

```cpp
#include "localllm_capi.h"
#include <stdio.h>

int main() {
    // æµ‹è¯•åŸºç¡€ API
    const char* error = nullptr;

    if (localllm_backend_init(&error) != LOCALLLM_SUCCESS) {
        printf("Backend init failed: %s\n", error);
        return 1;
    }

    printf("Backend initialized successfully\n");

    localllm_backend_free();
    printf("Backend freed successfully\n");

    return 0;
}
```

ç¼–è¯‘å¹¶è¿è¡Œï¼š

```bash
g++ -o test_backend test_backend.cpp -L./build -llocalllm
./test_backend
```

### R å±‚éªŒè¯

åˆ›å»º `test_upgrade.R`ï¼š

```r
library(localLLM)

test_upgrade <- function() {
  # 1. æµ‹è¯•åç«¯åˆå§‹åŒ–
  cat("Testing backend initialization...\n")
  tryCatch({
    backend_init()
    cat("âœ“ Backend init OK\n")
  }, error = function(e) {
    cat("âœ— Backend init FAILED:", conditionMessage(e), "\n")
    return(FALSE)
  })

  # 2. æµ‹è¯•æ¨¡å‹åŠ è½½
  cat("\nTesting model loading...\n")
  model_path <- "path/to/test/model.gguf"

  if (file.exists(model_path)) {
    tryCatch({
      model <- model_load(model_path, n_gpu_layers = 0)
      cat("âœ“ Model load OK\n")

      # 3. æµ‹è¯• tokenization
      cat("\nTesting tokenization...\n")
      tokens <- tokenize(model, "Hello world", add_special = TRUE)
      cat("âœ“ Tokenize OK, tokens:", length(tokens), "\n")

      # 4. æµ‹è¯•ä¸Šä¸‹æ–‡åˆ›å»º
      cat("\nTesting context creation...\n")
      ctx <- context_create(model, n_ctx = 512, n_threads = 4)
      cat("âœ“ Context create OK\n")

      # 5. æµ‹è¯•ç”Ÿæˆ
      cat("\nTesting generation...\n")
      result <- generate(ctx, "Test", max_tokens = 5)
      cat("âœ“ Generate OK\n")
      cat("Result:", result, "\n")

      cat("\n=== ALL TESTS PASSED ===\n")
      return(TRUE)

    }, error = function(e) {
      cat("âœ— Test FAILED:", conditionMessage(e), "\n")
      return(FALSE)
    })
  } else {
    cat("Skipping tests - model file not found\n")
  }
}

# è¿è¡Œæµ‹è¯•
test_upgrade()
```

---

## ç»“è®º

### âœ… **å¯è¡Œæ€§è¯„ä¼°ï¼šé«˜åº¦å¯è¡Œ**

ä½ çš„ R package æ¶æ„è®¾è®¡éå¸¸å¥½ï¼Œå‡çº§åˆ° b7785 çš„é£é™©å¾ˆä½ï¼š

1. **R å±‚ä»£ç **: å®Œå…¨ä¸éœ€è¦ä¿®æ”¹
2. **C API æ¥å£**: å®Œå…¨ä¸éœ€è¦ä¿®æ”¹
3. **åç«¯åº“**: éœ€è¦é‡æ–°ç¼–è¯‘ï¼Œå¹¶åšå°‘é‡è°ƒæ•´ï¼ˆä¸»è¦æ˜¯ä½¿ç”¨é»˜è®¤å‚æ•°åˆå§‹åŒ–ï¼‰

### ğŸ¯ **æ¨èè¿ç§»ç­–ç•¥**

**ç¬¬ 1 æ­¥**: åœ¨æœ¬åœ°æµ‹è¯•ç¯å¢ƒä¸­åˆ‡æ¢åˆ° b7785 å¹¶é‡æ–°ç¼–è¯‘åç«¯åº“

**ç¬¬ 2 æ­¥**: è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼Œç¡®ä¿æ‰€æœ‰åŠŸèƒ½æ­£å¸¸

**ç¬¬ 3 æ­¥**: åœ¨ GitHub Releases ä¸­å‘å¸ƒæ–°çš„åç«¯åº“ï¼ˆæ ‡è®°ä¸º v1.2.0-b7785ï¼‰

**ç¬¬ 4 æ­¥**: æ›´æ–° R package çš„é»˜è®¤ä¸‹è½½é“¾æ¥ï¼ŒæŒ‡å‘æ–°çš„åç«¯åº“

**ç¬¬ 5 æ­¥**: å‘å¸ƒ R package æ–°ç‰ˆæœ¬ï¼ˆå¦‚ 1.2.0ï¼‰

### ğŸ“… **é¢„è®¡å·¥ä½œé‡**

- **ä»£ç ä¿®æ”¹**: 0.5 å¤©ï¼ˆä»…åç«¯åº“å‚æ•°åˆå§‹åŒ–ï¼‰
- **ç¼–è¯‘æµ‹è¯•**: 0.5 å¤©ï¼ˆå¤šå¹³å°ç¼–è¯‘ï¼‰
- **å›å½’æµ‹è¯•**: 1 å¤©ï¼ˆå…¨é¢æµ‹è¯•ï¼‰
- **æ–‡æ¡£æ›´æ–°**: 0.5 å¤©
- **æ€»è®¡**: **2-3 å¤©**

### ğŸš€ **ä¸‹ä¸€æ­¥è¡ŒåŠ¨**

1. æ£€æŸ¥ `backend/llama.cpp/` ç›®å½•ä¸­ä½ çš„åç«¯å®ç°ä»£ç ä½ç½®
2. ç¡®è®¤å“ªä¸ªæ–‡ä»¶å®ç°äº† `localllm_model_load` ç­‰å‡½æ•°
3. å¼€å§‹ä¿®æ”¹å‚æ•°åˆå§‹åŒ–ä»£ç 
4. ç¼–è¯‘å¹¶æµ‹è¯•

éœ€è¦æˆ‘å¸®ä½ æŸ¥æ‰¾åç«¯å®ç°ä»£ç çš„å…·ä½“ä½ç½®å—ï¼Ÿ
