% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/annotations.R
\name{explore}
\alias{explore}
\title{Compare multiple LLMs over a shared set of prompts}
\usage{
explore(
  models,
  instruction = NULL,
  prompts = NULL,
  engine = c("auto", "parallel", "single"),
  batch_size = 8L,
  reuse_models = FALSE,
  sink = NULL,
  progress = interactive(),
  clean = TRUE,
  keep_prompts = FALSE
)
}
\arguments{
\item{models}{Model definitions. Accepts either a named character vector
(names become `model_id`s) or a list where each element is a list with at
least `id` and `model` (path/URL). Each model entry can optionally specify
`instruction`, `generation` parameters, a custom `prompt_builder`, or a
`predictor` function for mock/testing scenarios.}

\item{instruction}{Default task instruction inserted into `spec` whenever a
model entry does not override it.}

\item{prompts}{One of: (1) a function (for example `function(spec)`)
that returns prompts (character vector or a data frame with a `prompt` column);
(2) a character vector of ready-made prompts; or (3) a template list with
entries such as `annotation_task`, `coding_rules`, `examples`,
`target_text`, `sample_id`, `output_format`, and optional
`data`/`text_col`/`id_col` keys. Template lists are rendered using the
built-in annotation format described in the README. When `NULL`, each model
must provide its own `prompt_builder`.}

\item{engine}{One of `"auto"`, `"parallel"`, or `"single"`. Controls whether
`generate_parallel()` or `generate()` is used under the hood.}

\item{batch_size}{Number of prompts to send per backend call when the
parallel engine is active. Must be >= 1.}

\item{reuse_models}{If `TRUE`, model/context handles stay alive for the
duration of the function (useful when exploring lots of prompts). When
`FALSE` (default) handles are released after each model to minimise peak
memory usage.}

\item{sink}{Optional function that accepts `(chunk, model_id)` and is invoked
after each model finishes. This makes it easy to stream intermediate
results to disk via helpers such as [annotation_sink_csv()].}

\item{progress}{Whether to print progress messages for each model/batch.}

\item{clean}{Forwarded to `generate()`/`generate_parallel()` to remove control
tokens from the outputs.}

\item{keep_prompts}{If `TRUE`, the generated prompts are preserved in the
long-format output (useful for audits). Defaults to `FALSE`.}
}
\value{
A list with elements `annotations` (long table) and `matrix` (wide
  annotation matrix). When `sink` is supplied the `annotations` and `matrix`
  entries are set to `NULL` to avoid duplicating the streamed output.
}
\description{
`explore()` orchestrates running several models over the same prompts,
captures their predictions, and returns both long and wide annotation
tables that can be fed into confusion-matrix and reliability helpers.
}
